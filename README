put this demo dir to the bmnnsdk2-bm1684_vSA5 and enter docker, and init bmnnsdk

==========================
script usage:

export yolov5 offline model of torchscript format, the exported model will be in data/models
$ script/gen_pytorch.sh [img_size] [batch_size] # by default, img_size=640 batch_size=1

generate fp32 bmodel for running on device, pytorch model must exist in data/models
$ script/gen_fp32_bmodel.sh [img_size] [batch_size] # by default, img_size=640 batch_size=1

generate fix8b bmodel for running on device, pytorch model must exist in data/models
$ script/gen_fix8b_bmodel.sh dataset_dir [iteration] [img_size] [batch_size] # by default, iteration=1 img_size=640 batch_size=1


==========================
cpp demo usage:

1. for pcie platform
compile demo program in docker
	$ cd yolov5_demo/cpp
	$ make -f Makefile.pcie top_dir=/workspace #will generate yolov5_demo.pcie
then put yolov5_demo.pcie and yolov5_demo/data dir on pcie host with bm1684
  $ ./yolov5_demo.pcie
  $ ./yolov5_demo.pcie --input=path/to/image
  $ ./yolov5_demo.pcie --help # see detail help info

2. for SE5/arm
compile demo program in docker
	$ cd yolov5_demo/cpp
	$ make -f Makefile.arm top_dir=/workspace #will generate yolov5_demo.arm
then put yolov5_demo.arm and yolov5_demo/data dir on SE5
  $ ./yolov5_demo.arm
  $ ./yolov5_demo.arm --input=path/to/image
  $ ./yolov5_demo.arm --help # see detail help info

==========================
python demo usage:

on the host with opencv and pytorch
$ cd yolov5_demo
$ python3 python/pytorch.py
$ python3 python/pytorch.py -h #show help info

on the pcie host with bm1684
$ cd yolov5_demo
$ python3 python/sail.py 
$ python3 python/sail.py -h #show help info

on SE5
$ cd yolov5_demo
$ export PYTHONPATH=$PYTHONPATH:/system/lib
$ python3 python/sail.py 
$ python3 python/sail.py -h # show help info

